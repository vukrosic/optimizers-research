import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import math
import random
import numpy as np
from datasets import load_dataset
from tqdm import tqdm
import time
from transformers import AutoTokenizer
from dataclasses import dataclass
from typing import List, Optional, Tuple
import warnings
import os
import pickle
from torchtune.modules import RotaryPositionalEmbeddings
warnings.filterwarnings('ignore')

def set_seed(seed: int = 42):
    """Set all random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"üå± Set all seeds to {seed}")

@dataclass
class MoEModelConfig:
    # Model architecture
    d_model: int = 384
    n_heads: int = 8
    n_layers: int = 6
    d_ff: int = 1536
    batch_size: int = 24
    max_steps: int = 3000

    # Training parameters
    gradient_accumulation_steps: int = 4
    muon_lr: float = 0.01

    # Data parameters
    max_seq_len: int = 512
    num_documents: int = 2000
    max_tokens: int = 500000

    # Evaluation
    eval_every: int = 500
    eval_steps: int = 100

    # Regularization
    weight_decay: float = 0.1
    dropout: float = 0.1
    grad_clip: float = 1.0

    # Technical
    use_amp: bool = True
    vocab_size: Optional[int] = None
    log_milestones: Tuple[int, ...] = (2000, 5000, 10000)

    # MoE specific parameters
    num_experts: int = 8
    expert_top_k: int = 2
    load_balancing_weight: float = 0.01

    def __post_init__(self):
        self.d_k = self.d_model // self.n_heads
        assert self.d_model % self.n_heads == 0, "d_model must be divisible by n_heads"

@torch.compile
def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:
    """Newton-Schulz iteration to compute the zeroth power / orthogonalization of G."""
    assert G.ndim >= 2
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()

    if G.size(-2) > G.size(-1):
        X = X.mT

    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT

    return X

class Muon(torch.optim.Optimizer):
    """Muon - MomentUm Orthogonalized by Newton-schulz"""
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            for p in group["params"]:
                if p.grad is None:
                    continue

                g = p.grad
                state = self.state[p]

                if "momentum_buffer" not in state:
                    state["momentum_buffer"] = torch.zeros_like(g)

                buf = state["momentum_buffer"]
                buf.lerp_(g, 1 - group["momentum"])
                g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"])
                p.add_(g.view_as(p), alpha=-group["lr"] * max(1, p.size(-2) / p.size(-1))**0.5)
	
def load_and_cache_data(config: MoEModelConfig, cache_dir: str = "data_cache"):
    """Load and cache tokenized data to avoid reprocessing"""
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = f"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl"

    # Check if cached data exists
    if os.path.exists(cache_file):
        print(f"üì¶ Loading cached data from {cache_file}")
        with open(cache_file, 'rb') as f:
            cached_data = pickle.load(f)

        texts = cached_data['texts']
        tokenizer = cached_data['tokenizer']
        tokens = cached_data['tokens']
        config.vocab_size = tokenizer.vocab_size

        print(f"‚úÖ Loaded {len(texts)} documents, {len(tokens):,} tokens from cache")
        return texts, tokenizer, tokens

    print(f"üîÑ Processing new data (will cache for future use)")

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM-135M", token=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Load dataset
    dataset = load_dataset("HuggingFaceTB/smollm-corpus", "cosmopedia-v2", split="train", streaming=True, token=False)

    texts = []
    for i, item in enumerate(dataset):
        if i >= config.num_documents:
            break
        texts.append(item["text"][:3000])

    print(f"Loaded {len(texts)} documents")

    # Tokenize
    print("Tokenizing texts...")
    all_tokens = []
    for text in tqdm(texts, desc="Tokenizing"):
        tokens = tokenizer.encode(text, add_special_tokens=False)
        all_tokens.extend(tokens)

    tokens = all_tokens[:config.max_tokens]
    print(f"Using {len(tokens):,} tokens")
    config.vocab_size = tokenizer.vocab_size

    # Cache the processed data
    cached_data = {'texts': texts, 'tokenizer': tokenizer, 'tokens': tokens}
    with open(cache_file, 'wb') as f:
        pickle.dump(cached_data, f)

    print(f"üíæ Cached data to {cache_file}")
    return texts, tokenizer, tokens

class TextTokenDataset(Dataset):
    def __init__(self, tokens: List[int], seq_len: int = 512):
        self.tokens = tokens
        self.seq_len = seq_len

    def __len__(self):
        return max(0, len(self.tokens) - self.seq_len)

    def __getitem__(self, idx):
        x = torch.tensor(self.tokens[idx:idx + self.seq_len], dtype=torch.long)
        y = torch.tensor(self.tokens[idx + 1:idx + self.seq_len + 1], dtype=torch.long)
        return x, y

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        self.rope = RotaryPositionalEmbeddings(dim=dim, max_seq_len=max_seq_len, base=10000)

    def forward(self, x_BTHD: torch.Tensor):
        # x_BTHD shape: [B, T, H, D] - need to convert to [B, T, H, D] for torchtune
        # torchtune expects [batch, seq_len, num_heads, head_dim]
        # Our input is already [B, T, H, D] which matches torchtune's expectation
        return self.rope(x_BTHD)

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, n_heads: int, max_seq_len: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.qkv = nn.Linear(d_model, d_model * 3, bias=False)
        self.w_o = nn.Linear(d_model, d_model, bias=False)
        self.rotary = Rotary(self.d_k, max_seq_len)
        self.dropout = dropout

    def forward(self, x):
        batch_size, seq_len = x.size(0), x.size(1)
        # B, T = x.size(0), x.size(1)
        # qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)
        # Q, K, V = qkv[0], qkv[1], qkv[2]  # [B, H, T, D]

        qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)
        qkv = qkv.permute(2, 0, 3, 1, 4)
        Q, K, V = qkv[0], qkv[1], qkv[2] # [B, H, T, D]

        # Q = self.rotary(Q)
        # K = self.rotary(K)
        # Apply RoPE on [B, T, H, D]
        Q = self.rotary(Q.transpose(1, 2)).transpose(1, 2)
        K = self.rotary(K.transpose(1, 2)).transpose(1, 2)

        attn_output = F.scaled_dot_product_attention(
            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0
        )
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)
        # attn_output = attn_output.transpose(1, 2).reshape(B, T, self.d_model)
        return self.w_o(attn_output)



class Expert(nn.Module):
    """Single expert network (essentially a FeedForward layer)"""
    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff, bias=False)
        self.linear2 = nn.Linear(d_ff, d_model, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.linear2(self.dropout(F.silu(self.linear1(x))))

class TopKRouter(nn.Module):
    """Router that selects top-k experts for each token"""
    def __init__(self, d_model: int, num_experts: int, top_k: int = 2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.gate = nn.Linear(d_model, num_experts, bias=False)
        self.noise_std = 0.1  # Standard deviation for noise during training

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            x: Input tensor [batch_size, seq_len, d_model]

        Returns:
            - router_weights: Softmax weights for selected experts [batch_size, seq_len, top_k]
            - expert_indices: Indices of selected experts [batch_size, seq_len, top_k]
            - router_probs: Full probability distribution over experts (for load balancing loss)
        """
        batch_size, seq_len, d_model = x.shape

        # Compute router logits
        router_logits = self.gate(x)  # [batch_size, seq_len, num_experts]

        # Add noise during training for exploration
        if self.training and self.noise_std > 0:
            noise = torch.randn_like(router_logits) * self.noise_std
            router_logits = router_logits + noise

        # Get full probability distribution (for load balancing loss)
        router_probs = F.softmax(router_logits, dim=-1)

        # Select top-k experts
        top_k_logits, top_k_indices = torch.topk(router_logits, self.top_k, dim=-1)
        top_k_weights = F.softmax(top_k_logits, dim=-1)

        return top_k_weights, top_k_indices, router_probs

class MixtureOfExperts(nn.Module):
    """Mixture of Experts layer with top-k routing"""
    def __init__(
        self,
        d_model: int,
        d_ff: int,
        num_experts: int = 8,
        top_k: int = 2,
        dropout: float = 0.1,
        load_balancing_weight: float = 0.01
    ):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.load_balancing_weight = load_balancing_weight

        # Create experts
        self.experts = nn.ModuleList([
            Expert(d_model, d_ff, dropout) for _ in range(num_experts)
        ])

        # Create router
        self.router = TopKRouter(d_model, num_experts, top_k)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Args:
            x: Input tensor [batch_size, seq_len, d_model]

        Returns:
            - output: MoE output [batch_size, seq_len, d_model]
            - aux_loss: Load balancing auxiliary loss (only during training)
        """
        batch_size, seq_len, d_model = x.shape

        # Get routing decisions
        router_weights, expert_indices, router_probs = self.router(x)

        # Initialize output tensor
        output = torch.zeros_like(x)

        # Process each expert
        for expert_idx in range(self.num_experts):
            # Find tokens routed to this expert
            expert_mask = (expert_indices == expert_idx).any(dim=-1)  # [batch_size, seq_len]

            if expert_mask.any():
                # Get tokens for this expert
                expert_input = x[expert_mask]  # [num_tokens, d_model]

                # Apply expert
                expert_output = self.experts[expert_idx](expert_input)

                # Get weights for this expert - CORRECTED APPROACH
                # First get the mask for this expert's positions
                mask_for_expert = (expert_indices == expert_idx)  # [batch, seq, top_k]
                # Find which position (0 or 1) this expert appears in for relevant tokens
                positions = mask_for_expert[expert_mask].float().argmax(dim=-1)
                # Gather weights only for relevant tokens
                expert_weights = router_weights[expert_mask].gather(
                    -1, positions.unsqueeze(-1)
                ).squeeze(-1)

                # Add weighted expert output to result
                output[expert_mask] += expert_weights.unsqueeze(-1) * expert_output

        # Compute load balancing loss during training
        aux_loss = None
        if self.training:
            aux_loss = self._compute_load_balancing_loss(router_probs, expert_indices)

        return output, aux_loss

    def _compute_load_balancing_loss(
        self,
        router_probs: torch.Tensor,
        expert_indices: torch.Tensor
    ) -> torch.Tensor:
        """
        Compute auxiliary loss to ensure balanced expert usage.
        This encourages the router to distribute tokens evenly across experts.
        """
        # Compute the fraction of tokens routed to each expert
        expert_mask = F.one_hot(expert_indices, num_classes=self.num_experts).float()
        tokens_per_expert = expert_mask.sum(dim=[0, 1, 2]) / expert_mask.sum()

        # Compute the average probability of routing to each expert
        router_prob_mean = router_probs.mean(dim=[0, 1])

        # Load balancing loss encourages uniform distribution
        aux_loss = torch.sum(tokens_per_expert * router_prob_mean) * self.num_experts

        return aux_loss * self.load_balancing_weight

class MoETransformerBlock(nn.Module):
    """Transformer block with MoE"""
    def __init__(
        self,
        d_model: int,
        n_heads: int,
        d_ff: int,
        max_seq_len: int,
        num_experts: int = 8,
        top_k: int = 2,
        dropout: float = 0.1
    ):
        super().__init__()

        # Attention layer
        self.attention = MultiHeadAttention(d_model, n_heads, max_seq_len, dropout)

        # MoE layer
        self.feed_forward = MixtureOfExperts(
            d_model, d_ff, num_experts, top_k, dropout
        )

        # Normalization layers
        self.norm1 = nn.RMSNorm(d_model)
        self.norm2 = nn.RMSNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Self-attention
        attn_out = self.attention(self.norm1(x))
        x = x + self.dropout(attn_out)

        # MoE feed-forward
        ff_out, aux_loss = self.feed_forward(self.norm2(x))
        x = x + self.dropout(ff_out)
        return x, aux_loss


class MoEMinimalLLM(nn.Module):
    """Minimal LLM with Mixture of Experts"""
    def __init__(self, config: MoEModelConfig):
        super().__init__()
        self.config = config

        # Token embeddings
        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)
        self.position_dropout = nn.Dropout(config.dropout)

        # Transformer blocks with MoE
        self.transformer_blocks = nn.ModuleList([
            MoETransformerBlock(
                config.d_model,
                config.n_heads,
                config.d_ff,
                config.max_seq_len,
                config.num_experts,
                config.expert_top_k,
                config.dropout
            )
            for i in range(config.n_layers)
        ])

        # Output layers
        self.norm = nn.RMSNorm(config.d_model)
        self.output_dropout = nn.Dropout(config.dropout)

        # Language modeling head (tied with embeddings)
        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)
        self.lm_head.weight = self.token_embedding.weight

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, x, return_aux_loss=True):
        # Token embeddings
        x = self.token_embedding(x) * math.sqrt(self.config.d_model)
        x = self.position_dropout(x)

        # Collect auxiliary losses from MoE layers
        aux_losses = []

        # Pass through transformer blocks
        for block in self.transformer_blocks:
            x, aux_loss = block(x)
            if aux_loss is not None and return_aux_loss:
                aux_losses.append(aux_loss)

        # Output projection
        x = self.norm(x)
        x = self.output_dropout(x)
        logits = self.lm_head(x)

        # Combine auxiliary losses
        total_aux_loss = sum(aux_losses) if aux_losses else None

        if return_aux_loss:
            return logits, total_aux_loss
        return logits

def evaluate_model(model: nn.Module, val_loader: DataLoader, config: MoEModelConfig):
    """Evaluate model performance"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    total_correct = 0

    device = next(model.parameters()).device

    with torch.no_grad():
        for i, (x, y) in enumerate(val_loader):
            if i >= config.eval_steps:
                break
            x, y = x.to(device), y.to(device)

            with autocast(enabled=config.use_amp):
                # MoE model evaluation
                logits = model(x, return_aux_loss=False)  # Don't return aux loss during eval
                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))

            total_loss += loss.item() * y.numel()
            total_tokens += y.numel()

            predictions = logits.argmax(dim=-1)
            total_correct += (predictions == y).sum().item()

    avg_loss = total_loss / total_tokens
    accuracy = total_correct / total_tokens
    perplexity = math.exp(min(avg_loss, 20))

    model.train()
    return {'val_loss': avg_loss, 'val_accuracy': accuracy, 'val_perplexity': perplexity}

def setup_muon_optimizer(model: nn.Module, config: MoEModelConfig):
    """Setup Muon optimizer with hybrid approach"""
    muon_params = []
    adamw_params = []

    for name, param in model.named_parameters():
        if (param.ndim == 2 and 
            'token_embedding' not in name and 
            'norm' not in name and 
            param.requires_grad):
            muon_params.append(param)
        else:
            adamw_params.append(param)

    print(f"  Muon parameters: {sum(p.numel() for p in muon_params):,}")
    print(f"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}")

    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)
    adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)

    return [muon_optimizer, adamw_optimizer]


def train_moe_model(config: MoEModelConfig, train_loader: DataLoader, val_loader: DataLoader):
    """Train the MoE model"""
    print(f"\nüöÄ Training MoE model with {config.num_experts} experts (top-{config.expert_top_k})")

    # Initialize model
    set_seed(42)
    model = MoEMinimalLLM(config)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    active_params = sum(p.numel() for n, p in model.named_parameters()
                       if 'expert' not in n)
    expert_params = total_params - active_params

    print(f"  üìä Total parameters: {total_params:,}")
    print(f"  üìä Active parameters: {active_params:,}")
    print(f"  üìä Expert parameters: {expert_params:,}")
    print(f"  üìä Parameter efficiency: {active_params/total_params:.1%} active per forward pass")

    # Setup optimizers
    optimizers = setup_muon_optimizer(model, config)

    # Learning rate schedule
    schedulers = []
    for optimizer in optimizers:
        warmup_steps = config.max_steps // 20
        def lr_lambda(step):
            if step < warmup_steps:
                return step / warmup_steps
            else:
                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)
                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))

        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        schedulers.append(scheduler)

    scaler = GradScaler() if config.use_amp else None

    # Training loop
    model.train()
    step = 0
    pbar = tqdm(total=config.max_steps, desc="Training MoE")

    while step < config.max_steps:
        for batch_idx, (x, y) in enumerate(train_loader):
            if step >= config.max_steps:
                break

            x, y = x.to(device), y.to(device)

            # Forward pass
            if config.use_amp:
                with autocast():
                    logits, aux_loss = model(x, return_aux_loss=True)
                    ce_loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))

                    # Combine main loss and auxiliary loss
                    total_loss = ce_loss
                    if aux_loss is not None:
                        total_loss = total_loss + aux_loss

                    loss = total_loss / config.gradient_accumulation_steps
                scaler.scale(loss).backward()
            else:
                logits, aux_loss = model(x, return_aux_loss=True)
                ce_loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))

                total_loss = ce_loss
                if aux_loss is not None:
                    total_loss = total_loss + aux_loss

                loss = total_loss / config.gradient_accumulation_steps
                loss.backward()

            # Optimizer step
            if (step + 1) % config.gradient_accumulation_steps == 0:
                if config.use_amp:
                    for optimizer in optimizers:
                        scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)

                    for optimizer in optimizers:
                        scaler.step(optimizer)
                        optimizer.zero_grad()
                    for scheduler in schedulers:
                        scheduler.step()
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
                    for optimizer in optimizers:
                        optimizer.step()
                        optimizer.zero_grad()
                    for scheduler in schedulers:
                        scheduler.step()

            # Logging
            if step % 100 == 0:
                with torch.no_grad():
                    predictions = logits.argmax(dim=-1)
                    accuracy = (predictions == y).float().mean().item()
                    current_loss = ce_loss.item()
                    perplexity = math.exp(min(current_loss, 20))

                pbar.set_postfix({
                    'loss': f'{current_loss:.4f}',
                    'aux': f'{aux_loss.item() if aux_loss is not None else 0:.4f}',
                    'acc': f'{accuracy:.3f}',
                    'ppl': f'{perplexity:.1f}'
                })

            # Evaluation
            if step % config.eval_every == 0 and step > 0:
                eval_metrics = evaluate_model(model, val_loader, config)
                print(f"\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, "
                      f"Val Acc: {eval_metrics['val_accuracy']:.4f}, "
                      f"Val PPL: {eval_metrics['val_perplexity']:.2f}")

            # Milestone evaluations
            if step in getattr(config, 'log_milestones', ()):    
                eval_metrics = evaluate_model(model, val_loader, config)
                print(f"\nüß™ Milestone {step}: Val Loss: {eval_metrics['val_loss']:.4f}")

            step += 1
            if step % 100 == 0:
                pbar.update(100)

    pbar.close()

    # Final evaluation
    final_eval = evaluate_model(model, val_loader, config)
    print(f"\nüìä Final Results:")
    print(f"   Val Loss: {final_eval['val_loss']:.4f}")
    print(f"   Val Accuracy: {final_eval['val_accuracy']:.4f}")
    print(f"   Val Perplexity: {final_eval['val_perplexity']:.2f}")

    return model, final_eval

# =============================================================================
# üî¨ ADVANCED OPTIMIZER RESEARCH FOR FASTER LLM CONVERGENCE
# =============================================================================

class SophiaG(torch.optim.Optimizer):
    """
    Sophia-G optimizer with Hutchinson trace estimator for diagonal Hessian approximation.
    More efficient than full Hessian but captures curvature information for faster convergence.
    """
    def __init__(self, params, lr=1e-4, betas=(0.965, 0.99), rho=0.04, weight_decay=1e-1, 
                 maximize=False, capturable=False):
        defaults = dict(lr=lr, betas=betas, rho=rho, weight_decay=weight_decay, 
                       maximize=maximize, capturable=capturable)
        super().__init__(params, defaults)

    def __setstate__(self, state):
        super().__setstate__(state)
        for group in self.param_groups:
            group.setdefault('maximize', False)
            group.setdefault('capturable', False)

    @torch.no_grad()
    def step(self, closure=None, bs=5120):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            params_with_grad = []
            grads = []
            exp_avgs = []
            exp_hessian_diag_sqs = []
            state_steps = []
            beta1, beta2 = group['betas']

            for p in group['params']:
                if p.grad is not None:
                    params_with_grad.append(p)
                    if p.grad.dtype in {torch.float16, torch.bfloat16}:
                        grads.append(p.grad.float())
                    else:
                        grads.append(p.grad)

                    state = self.state[p]
                    # Lazy state initialization
                    if len(state) == 0:
                        state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \
                            if group['capturable'] else 0
                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format).float()
                        state['hessian'] = torch.zeros_like(p, memory_format=torch.preserve_format).float()

                    exp_avgs.append(state['exp_avg'])
                    exp_hessian_diag_sqs.append(state['hessian'])

                    if group['capturable']:
                        state_steps.append(state['step'])
                    else:
                        state_steps.append(torch.tensor(float(state['step'])))

            sophia_update(params_with_grad,
                         grads,
                         exp_avgs,
                         exp_hessian_diag_sqs,
                         state_steps,
                         bs=bs,
                         beta1=beta1,
                         beta2=beta2,
                         rho=group['rho'],
                         lr=group['lr'],
                         weight_decay=group['weight_decay'],
                         maximize=group['maximize'],
                         capturable=group['capturable'])

        return loss

def sophia_update(params, grads, exp_avgs, exp_hessian_diag_sqs, state_steps,
                  bs: int, beta1: float, beta2: float, rho: float, lr: float,
                  weight_decay: float, maximize: bool, capturable: bool):
    if not all(isinstance(t, torch.Tensor) for t in state_steps):
        raise RuntimeError("API has changed, `state_steps` argument must contain a list of singleton tensors")

    for i, param in enumerate(params):
        grad = grads[i] if not maximize else -grads[i]
        exp_avg = exp_avgs[i]
        hut_trace = exp_hessian_diag_sqs[i]
        step_t = state_steps[i]

        if capturable:
            assert param.device == step_t.device == exp_avg.device == hut_trace.device == grad.device

        if torch.is_complex(param):
            grad = torch.view_as_real(grad)
            exp_avg = torch.view_as_real(exp_avg)
            hut_trace = torch.view_as_real(hut_trace)
            param = torch.view_as_real(param)

        # update step
        step_t += 1

        # Perform stepweight decay
        param.mul_(1 - lr * weight_decay)

        # Decay the first and second moment running average coefficient
        exp_avg.lerp_(grad, 1 - beta1)

        if len(params) > 1:
            zeta = torch.randint_like(grad, high=2, dtype=grad.dtype) * 2.0 - 1.0
        else:
            zeta = torch.randint_like(grad, high=2, dtype=grad.dtype) * 2.0 - 1.0

        # Hutchinson trace Hessian diagonal estimate
        h = grad * zeta
        hut_trace.lerp_(h * h, 1 - beta2)

        bias_correction1 = 1 - beta1 ** step_t.item()
        bias_correction2 = 1 - beta2 ** step_t.item()

        k = group_product(exp_avgs, exp_hessian_diag_sqs) / bs
        u = exp_avg / bias_correction1 / (torch.sqrt(hut_trace / bias_correction2) + rho).clamp_(min=1e-15)
        param.add_(u * k, alpha=-lr)

def group_product(tensor_list, tensor_list2):
    return sum([torch.sum(tensor1 * tensor2) for tensor1, tensor2 in zip(tensor_list, tensor_list2)])


class Lion(torch.optim.Optimizer):
    """
    Lion optimizer - EvoLved Sign Momentum.
    Uses sign of momentum for updates, which can lead to faster convergence with better generalization.
    """
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):
        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad
                state = self.state[p]
                
                # State Initialization
                if len(state) == 0:
                    state['exp_avg'] = torch.zeros_like(p)

                exp_avg = state['exp_avg']
                beta1, beta2 = group['betas']

                # Weight decay
                if group['weight_decay'] > 0:
                    p.add_(p, alpha=-group['weight_decay'] * group['lr'])

                # Update with sign of interpolated momentum
                update = exp_avg * beta1 + grad * (1 - beta1)
                p.add_(torch.sign(update), alpha=-group['lr'])

                # Decay the momentum running average coefficient
                exp_avg.lerp_(grad, 1 - beta2)

        return loss


class FocalLoss(nn.Module):
    """
    Focal Loss for addressing class imbalance and hard examples.
    Reduces loss for well-classified examples and focuses on hard examples.
    """
    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class LabelSmoothingCrossEntropy(nn.Module):
    """
    Label Smoothing Cross Entropy - prevents overconfidence and improves generalization.
    """
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        n_class = pred.size(1)
        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)
        smooth_one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class
        loss = torch.sum(-smooth_one_hot * F.log_softmax(pred, 1), 1)
        return loss.mean()


class SwishActivation(nn.Module):
    """Swish activation: x * sigmoid(x) - often better than ReLU for deep networks"""
    def forward(self, x):
        return x * torch.sigmoid(x)


class GeGLU(nn.Module):
    """Gated Linear Unit with GELU activation - improves information flow"""
    def __init__(self, dim_in, dim_out):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out * 2)

    def forward(self, x):
        x, gate = self.proj(x).chunk(2, dim=-1)
        return x * F.gelu(gate)


class AdaptiveBatchSizeTrainer:
    """
    Adaptive batch size training - automatically adjusts batch size based on gradient variance.
    """
    def __init__(self, initial_batch_size=32, min_batch_size=8, max_batch_size=128):
        self.batch_size = initial_batch_size
        self.min_batch_size = min_batch_size
        self.max_batch_size = max_batch_size
        self.grad_var_history = []
        self.adjustment_threshold = 0.1

    def should_increase_batch_size(self, grad_variance):
        if len(self.grad_var_history) < 10:
            self.grad_var_history.append(grad_variance)
            return False
        
        recent_variance = np.mean(self.grad_var_history[-10:])
        if grad_variance < recent_variance * (1 - self.adjustment_threshold):
            return True
        return False

    def should_decrease_batch_size(self, grad_variance):
        if len(self.grad_var_history) < 10:
            return False
        
        recent_variance = np.mean(self.grad_var_history[-10:])
        if grad_variance > recent_variance * (1 + self.adjustment_threshold):
            return True
        return False

    def update_batch_size(self, grad_variance):
        self.grad_var_history.append(grad_variance)
        
        if self.should_increase_batch_size(grad_variance) and self.batch_size < self.max_batch_size:
            self.batch_size = min(self.batch_size * 2, self.max_batch_size)
            print(f"üìà Increased batch size to {self.batch_size}")
        elif self.should_decrease_batch_size(grad_variance) and self.batch_size > self.min_batch_size:
            self.batch_size = max(self.batch_size // 2, self.min_batch_size)
            print(f"üìâ Decreased batch size to {self.batch_size}")


@dataclass
class AdvancedMoEModelConfig(MoEModelConfig):
    """Extended configuration with advanced training techniques"""
    # Optimizer selection
    optimizer_type: str = "muon"  # "muon", "sophia", "lion", "adamw"
    
    # Loss function improvements
    use_focal_loss: bool = False
    focal_alpha: float = 1.0
    focal_gamma: float = 2.0
    use_label_smoothing: bool = True
    label_smoothing: float = 0.1
    
    # Architecture improvements  
    use_swish_activation: bool = True
    use_geglu: bool = True
    
    # Training dynamics
    use_adaptive_batch_size: bool = False
    use_cosine_restarts: bool = True
    restart_cycles: int = 3
    
    # Advanced regularization
    dropout_attention: float = 0.1
    dropout_feedforward: float = 0.1
    stochastic_depth_rate: float = 0.1
    
    # Learning rate scheduling
    use_polynomial_decay: bool = False
    polynomial_power: float = 1.0
    
    # Gradient analysis
    track_gradient_norms: bool = True
    gradient_norm_clip: float = 1.0


class ExperimentTracker:
    """Track experiments and compare different approaches"""
    def __init__(self):
        self.experiments = []
        self.current_experiment = None

    def start_experiment(self, name, config):
        self.current_experiment = {
            'name': name,
            'config': config,
            'metrics': [],
            'start_time': time.time(),
            'gradient_norms': [],
            'learning_rates': []
        }

    def log_metrics(self, step, loss, accuracy, lr=None, grad_norm=None):
        if self.current_experiment:
            self.current_experiment['metrics'].append({
                'step': step,
                'loss': loss,
                'accuracy': accuracy,
                'timestamp': time.time()
            })
            if lr is not None:
                self.current_experiment['learning_rates'].append(lr)
            if grad_norm is not None:
                self.current_experiment['gradient_norms'].append(grad_norm)

    def end_experiment(self, final_metrics):
        if self.current_experiment:
            self.current_experiment['end_time'] = time.time()
            self.current_experiment['duration'] = (
                self.current_experiment['end_time'] - self.current_experiment['start_time']
            )
            self.current_experiment['final_metrics'] = final_metrics
            self.experiments.append(self.current_experiment)
            self.current_experiment = None

    def compare_experiments(self):
        """Compare all experiments and return best performing one"""
        if not self.experiments:
            return None
        
        best_exp = min(self.experiments, key=lambda x: x['final_metrics']['val_loss'])
        print(f"\nüèÜ EXPERIMENT COMPARISON:")
        print(f"{'='*80}")
        for i, exp in enumerate(self.experiments):
            status = "ü•á BEST" if exp == best_exp else f"#{i+1}"
            print(f"{status} {exp['name']}")
            print(f"   Final Loss: {exp['final_metrics']['val_loss']:.4f}")
            print(f"   Final Accuracy: {exp['final_metrics']['val_accuracy']:.4f}")
            print(f"   Training Time: {exp['duration']/60:.1f} min")
            print(f"   Optimizer: {exp['config'].optimizer_type}")
            print()
        
        return best_exp


def setup_advanced_optimizer(model: nn.Module, config: AdvancedMoEModelConfig):
    """Setup advanced optimizers based on configuration"""
    
    # Separate parameters for different optimizers
    matrix_params = []
    other_params = []
    
    for name, param in model.named_parameters():
        if (param.ndim == 2 and 
            'token_embedding' not in name and 
            'norm' not in name and 
            param.requires_grad):
            matrix_params.append(param)
        else:
            other_params.append(param)
    
    print(f"  Matrix parameters: {sum(p.numel() for p in matrix_params):,}")
    print(f"  Other parameters: {sum(p.numel() for p in other_params):,}")
    
    optimizers = []
    
    if config.optimizer_type == "sophia":
        if matrix_params:
            optimizers.append(SophiaG(matrix_params, lr=config.muon_lr, weight_decay=config.weight_decay))
        if other_params:
            optimizers.append(torch.optim.AdamW(other_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay))
    
    elif config.optimizer_type == "lion":
        if matrix_params:
            optimizers.append(Lion(matrix_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay))
        if other_params:
            optimizers.append(torch.optim.AdamW(other_params, lr=config.muon_lr*0.01, weight_decay=config.weight_decay))
    
    elif config.optimizer_type == "muon":
        if matrix_params:
            optimizers.append(Muon(matrix_params, lr=config.muon_lr, momentum=0.95))
        if other_params:
            optimizers.append(torch.optim.AdamW(other_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay))
    
    else:  # adamw
        all_params = matrix_params + other_params
        optimizers.append(torch.optim.AdamW(all_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay))
    
    return optimizers


def setup_advanced_scheduler(optimizers, config: AdvancedMoEModelConfig):
    """Setup advanced learning rate schedulers"""
    schedulers = []
    
    for optimizer in optimizers:
        if config.use_cosine_restarts:
            # Cosine annealing with warm restarts
            T_0 = config.max_steps // config.restart_cycles
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, T_0=T_0, T_mult=1, eta_min=config.muon_lr * 0.01
            )
        elif config.use_polynomial_decay:
            # Polynomial decay
            scheduler = torch.optim.lr_scheduler.PolynomialLR(
                optimizer, total_iters=config.max_steps, power=config.polynomial_power
            )
        else:
            # Standard cosine with warmup
            warmup_steps = config.max_steps // 20
            def lr_lambda(step):
                if step < warmup_steps:
                    return step / warmup_steps
                else:
                    progress = (step - warmup_steps) / (config.max_steps - warmup_steps)
                    return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))
            
            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        
        schedulers.append(scheduler)
    
    return schedulers


def get_advanced_loss_function(config: AdvancedMoEModelConfig):
    """Get advanced loss function based on configuration"""
    if config.use_focal_loss:
        return FocalLoss(alpha=config.focal_alpha, gamma=config.focal_gamma)
    elif config.use_label_smoothing:
        return LabelSmoothingCrossEntropy(smoothing=config.label_smoothing)
    else:
        return F.cross_entropy


def calculate_gradient_norm(model):
    """Calculate the norm of gradients for monitoring training dynamics"""
    total_norm = 0
    param_count = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.detach().data.norm(2)
            total_norm += param_norm.item() ** 2
            param_count += 1
    return (total_norm ** 0.5) / max(param_count, 1)


def train_advanced_moe_model(config: AdvancedMoEModelConfig, train_loader: DataLoader, 
                           val_loader: DataLoader, experiment_tracker: ExperimentTracker):
    """Advanced training with multiple optimization techniques"""
    experiment_name = f"Advanced_{config.optimizer_type}_{config.use_focal_loss}_{config.use_label_smoothing}"
    experiment_tracker.start_experiment(experiment_name, config)
    
    print(f"\nüöÄ Training Advanced MoE model")
    print(f"   Optimizer: {config.optimizer_type}")
    print(f"   Loss function: {'Focal' if config.use_focal_loss else 'Label Smoothed' if config.use_label_smoothing else 'Standard'}")
    print(f"   Architecture: {'SwishGEGLU' if config.use_swish_activation and config.use_geglu else 'Standard'}")

    # Initialize model with advanced features
    set_seed(42)
    model = AdvancedMoEMinimalLLM(config)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # Setup advanced optimizers and schedulers
    optimizers = setup_advanced_optimizer(model, config)
    schedulers = setup_advanced_scheduler(optimizers, config)
    
    # Setup advanced loss function
    loss_fn = get_advanced_loss_function(config)
    
    # Adaptive batch size trainer
    adaptive_trainer = AdaptiveBatchSizeTrainer() if config.use_adaptive_batch_size else None
    
    scaler = GradScaler() if config.use_amp else None

    # Training loop
    model.train()
    step = 0
    best_val_loss = float('inf')
    grad_norm = None  # Initialize gradient norm variable
    pbar = tqdm(total=config.max_steps, desc=f"Training {config.optimizer_type}")

    while step < config.max_steps:
        for batch_idx, (x, y) in enumerate(train_loader):
            if step >= config.max_steps:
                break

            x, y = x.to(device), y.to(device)

            # Forward pass
            if config.use_amp:
                with autocast():
                    logits, aux_loss = model(x, return_aux_loss=True)
                    
                    # Advanced loss calculation
                    if config.use_focal_loss or config.use_label_smoothing:
                        ce_loss = loss_fn(logits.view(-1, config.vocab_size), y.view(-1))
                    else:
                        ce_loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))

                    total_loss = ce_loss
                    if aux_loss is not None:
                        total_loss = total_loss + aux_loss

                    loss = total_loss / config.gradient_accumulation_steps
                scaler.scale(loss).backward()
            else:
                logits, aux_loss = model(x, return_aux_loss=True)
                
                if config.use_focal_loss or config.use_label_smoothing:
                    ce_loss = loss_fn(logits.view(-1, config.vocab_size), y.view(-1))
                else:
                    ce_loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))

                total_loss = ce_loss
                if aux_loss is not None:
                    total_loss = total_loss + aux_loss

                loss = total_loss / config.gradient_accumulation_steps
                loss.backward()

            # Optimizer step
            if (step + 1) % config.gradient_accumulation_steps == 0:
                # Calculate gradient norm for tracking
                grad_norm = calculate_gradient_norm(model) if config.track_gradient_norms else None
                
                if config.use_amp:
                    for optimizer in optimizers:
                        scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_norm_clip)

                    for optimizer in optimizers:
                        scaler.step(optimizer)
                        optimizer.zero_grad()
                    for scheduler in schedulers:
                        scheduler.step()
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_norm_clip)
                    for optimizer in optimizers:
                        optimizer.step()
                        optimizer.zero_grad()
                    for scheduler in schedulers:
                        scheduler.step()

                # Adaptive batch size adjustment
                if adaptive_trainer and grad_norm is not None:
                    adaptive_trainer.update_batch_size(grad_norm)

            # Logging and tracking
            if step % 100 == 0:
                with torch.no_grad():
                    predictions = logits.argmax(dim=-1)
                    accuracy = (predictions == y).float().mean().item()
                    current_loss = ce_loss.item()
                    perplexity = math.exp(min(current_loss, 20))
                    
                    # Get current learning rate
                    current_lr = optimizers[0].param_groups[0]['lr']
                    
                    # Use the gradient norm calculated during optimizer step
                    # or calculate it now if tracking is enabled and not already done
                    if config.track_gradient_norms and grad_norm is None:
                        grad_norm = calculate_gradient_norm(model)
                    
                    experiment_tracker.log_metrics(
                        step, current_loss, accuracy, current_lr, grad_norm
                    )

                pbar.set_postfix({
                    'loss': f'{current_loss:.4f}',
                    'acc': f'{accuracy:.3f}',
                    'ppl': f'{perplexity:.1f}',
                    'lr': f'{current_lr:.2e}',
                    'opt': config.optimizer_type
                })

            # Evaluation
            if step % config.eval_every == 0 and step > 0:
                eval_metrics = evaluate_model(model, val_loader, config)
                print(f"\nStep {step}: Val Loss: {eval_metrics['val_loss']:.4f}, "
                      f"Val Acc: {eval_metrics['val_accuracy']:.4f}, "
                      f"Val PPL: {eval_metrics['val_perplexity']:.2f}")
                
                if eval_metrics['val_loss'] < best_val_loss:
                    best_val_loss = eval_metrics['val_loss']
                    print(f"üéØ New best validation loss: {best_val_loss:.4f}")

            step += 1
            if step % 100 == 0:
                pbar.update(100)

    pbar.close()

    # Final evaluation
    final_eval = evaluate_model(model, val_loader, config)
    experiment_tracker.end_experiment(final_eval)
    
    return model, final_eval


class AdvancedMoEMinimalLLM(MoEMinimalLLM):
    """Advanced MoE model with architectural improvements"""
    def __init__(self, config: AdvancedMoEModelConfig):
        # Temporarily modify config for parent class
        super().__init__(config)
        self.config = config
        
        # Replace feedforward layers with advanced versions if specified
        if config.use_geglu:
            for block in self.transformer_blocks:
                # Replace experts with GeGLU versions
                for expert in block.feed_forward.experts:
                    expert.linear1 = GeGLU(config.d_model, config.d_ff)


# Run comprehensive experiments
if __name__ == "__main__":
    print(f"üî¨ ADVANCED LLM TRAINING RESEARCH")
    print(f"{'='*80}")
    print(f"üîç Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # Set seed
    set_seed(42)

    # Load data
    temp_config = MoEModelConfig()
    texts, tokenizer, tokens = load_and_cache_data(temp_config)
    vocab_size = temp_config.vocab_size

    dataset = TextTokenDataset(tokens, temp_config.max_seq_len)
    val_size = len(dataset) // 10
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)
    )

    train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False, num_workers=2)

    # Initialize experiment tracker
    tracker = ExperimentTracker()

    # Define experiments to run
    experiments = [
        # Baseline with Muon
        AdvancedMoEModelConfig(
            d_model=384, n_heads=8, n_layers=6, d_ff=1536, batch_size=24,
            max_steps=500, eval_every=10000, vocab_size=vocab_size,
            num_experts=8, expert_top_k=2, load_balancing_weight=0.01,
            optimizer_type="muon", use_focal_loss=False, use_label_smoothing=False
        ),
        
        # Sophia optimizer
        AdvancedMoEModelConfig(
            d_model=384, n_heads=8, n_layers=6, d_ff=1536, batch_size=24,
            max_steps=500, eval_every=10000, vocab_size=vocab_size,
            num_experts=8, expert_top_k=2, load_balancing_weight=0.01,
            optimizer_type="sophia", use_focal_loss=False, use_label_smoothing=True
        ),
        
        # Lion optimizer with focal loss
        AdvancedMoEModelConfig(
            d_model=384, n_heads=8, n_layers=6, d_ff=1536, batch_size=24,
            max_steps=500, eval_every=10000, vocab_size=vocab_size,
            num_experts=8, expert_top_k=2, load_balancing_weight=0.01,
            optimizer_type="lion", use_focal_loss=True, use_label_smoothing=False,
            focal_alpha=1.0, focal_gamma=2.0
        ),
        
        # Advanced architecture + Muon
        AdvancedMoEModelConfig(
            d_model=384, n_heads=8, n_layers=6, d_ff=1536, batch_size=24,
            max_steps=500, eval_every=10000, vocab_size=vocab_size,
            num_experts=8, expert_top_k=2, load_balancing_weight=0.01,
            optimizer_type="muon", use_focal_loss=False, use_label_smoothing=True,
            use_swish_activation=True, use_geglu=True, use_cosine_restarts=True
        )
    ]

    # Run experiments
    for i, config in enumerate(experiments):
        print(f"\nüß™ EXPERIMENT {i+1}/{len(experiments)}: {config.optimizer_type.upper()}")
        print(f"{'='*60}")
        
        start_time = time.time()
        model, final_metrics = train_advanced_moe_model(config, train_loader, val_loader, tracker)
        total_time = time.time() - start_time
        
        print(f"\nüìä Experiment {i+1} Results:")
        print(f"   Training time: {total_time/60:.1f} minutes")
        print(f"   Final Loss: {final_metrics['val_loss']:.4f}")
        print(f"   Final Accuracy: {final_metrics['val_accuracy']:.4f}")
        print(f"   Final Perplexity: {final_metrics['val_perplexity']:.2f}")

    # Compare all experiments
    best_experiment = tracker.compare_experiments()
    
    print(f"\nüéØ RESEARCH CONCLUSION:")
    print(f"{'='*80}")
    print(f"Best approach: {best_experiment['name']}")
    print(f"Achieved validation loss: {best_experiment['final_metrics']['val_loss']:.4f}")
    print(f"Training time: {best_experiment['duration']/60:.1f} minutes")
    print(f"Recommended optimizer: {best_experiment['config'].optimizer_type}")
    print(f"{'='*80}")